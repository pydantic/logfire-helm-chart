# -- The secret used to pull down container images for pods
imagePullSecrets: []

ingress:
  # -- Enable Ingress Resource.
  # If you're not using an ingress resource, you still need to configure `tls`, `hostname`
  enabled: false
  # -- Enable TLS/HTTPS connections.  Required for CORS headers
  tls: false
  # -- The hostname used for Logfire
  hostname: "logfire.example.com"
  ingressClassName: nginx
  # -- Any annotations required.
  annotations: {}

# -- User provided postgres credentials
# containing `postgresDsn`, `postgresFFDsn`, `postgresIngestDsn` keys
postgresSecret:
  # -- Whether to use an existing secret
  enabled: false
  # -- Secret name
  name: ""

legacyIngest:
  enabled: true

# -- Custom annotations for migration Jobs
hooksAnnotations:
#  argocd.argoproj.io/hook: PreSync
#  argocd.argoproj.io/hook-delete-policy: HookSucceeded

# -- Postgres DSN used for `crud` database
postgresDsn: "postgresql://postgres:postgres@logfire-postgres:5432/crud"

# -- Postgres DSN used for `ff` database
postgresFFDsn: "postgresql://postgres:postgres@logfire-postgres:5432/ff"

# -- Postgres DSN used for `ingest` database
postgresIngestDsn: "postgresql://postgres:postgres@logfire-postgres:5432/ingest"

smtp:
  # -- Hostname of the SMTP server
  host:
  # -- Port of the SMTP server
  port: 25
  # -- SMTP username
  username:
  # -- SMTP password
  password:
  # -- Whether to use TLS
  use_tls: false

ai:
  # -- The AI provide and model to use. Prefix with the provider. I.e, For azure use `azure:gpt-4o`
  #
  # See https://ai.pydantic.dev/models/ for more info
  model:
  openAi:
    # -- The OpenAI API key
    apiKey:
  vertexAi:
    # -- The region for Vertex AI
    region:
  azureOpenAi:
    # -- The Azure OpenAI endpoint
    endpoint:
    # -- The Azure OpenAI API key
    apiKey:
    # -- The Azure OpenAI API version
    apiVersion:

# -- Configuration, autoscaling & resources for `logfire-dex` deployment
logfire-dex:
  # -- Number of replicas
  replicas: 1
  # -- Dex Config
  config:
    # -- Dex storage configuration, see https://dexidp.io/docs/configuration/storage/
    storage:
      type: postgres
      config:
        host: logfire-postgres
        port: 5432
        user: postgres
        database: dex
        password: postgres
        ssl:
          mode: disable
    # -- Dex auth connectors, see https://dexidp.io/docs/connectors/
    # redirectURI config option can be omitted, as it will be automatically generated
    # however if specified, the custom value will be honored
    connectors: []
  # -- resources
  resources:
    cpu: "1"
    memory: "1Gi"
#  autoscaling:
#    minReplicas: 2
#    maxReplicas: 4
#    memAverage: 65
#    cpuAverage: 20
#  pdb:
#    maxUnavailable: 1
#    minAvaliable: 1
# # -- Autoscaling & resources for the `logfire-ff-ingest` pod
logfire-ff-ingest:
  volumeClaimTemplates:
    storageClassName: standard
    storage: 64Gi
#   resources:
#     cpu: "2"
#     memory: "5Gi"
#   autoscaling:
#     minReplicas: 2
#     maxReplicas: 24
#     memAverage: 85
#     cpuAverage: 50
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# # -- Autoscaling & resources for the `logfire-backend` pod
# logfire-backend:
#   # -- Number of pod replicas
#   replicas: 2
#   # -- Resource limits and allocations
#   resources:
#     cpu: "2"
#     memory: "2Gi"
#   # -- Autoscaler settings
#   autoscaling:
#     minReplicas: 2
#     maxReplicas: 4
#     memAverage: 65
#     cpuAverage: 20
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# # -- Autoscaling & resources for the query api pods
# logfire-ff-query-api:
#   queryParallelism: 4
#   replicas: 2
#   resources:
#     cpu: "2"
#     memory: "2Gi"
#   autoscaling:
#     minReplicas: 2
#     maxReplicas: 8
#     memAverage: 65
#     cpuAverage: 20
#  pdb:
#    maxUnavailable: 1
#    minAvaliable: 1
#
# # -- Autoscaling & resources for the cache pods
# logfire-ff-cache:
#   replicas: 2
#   # -- How much disk storage is used for caching
#   cacheStorage: "10Gi"
#   # -- Which storage class is used for caching
#   cacheStorageClassName: "my-storage-class"
#   resources:
#     cpu: "4"
#     memory: "8Gi"
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# logfire-ff-conhash-cache:
#   replicas: 2
#   resources:
#     cpu: "1"
#     memory: "1Gi"
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# logfire-ff-ingest-api:
#   resources:
#     cpu: "1"
#     memory: "1Gi"
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# logfire-ff-ingest-worker:
#   replicas: 6
#   resources:
#     cpu: "1"
#     memory: "2Gi"
#   autoscaling:
#     minReplicas: 6
#     maxReplicas: 24
#     memAverage: 25
#     cpuAverage: 15
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#
# logfire-ff-maintenance-worker:
#   jobParallelism: 32
#   downloadParallelism: 32
#   replicas: 2
#   datafusionMemory: "3072MB"
#   compactionTiers:
#     - size_threshold_bytes: "1KB"
#       count_threshold: 10
#     - size_threshold_bytes: "10KB"
#       count_threshold: 10
#     - size_threshold_bytes: "100KB"
#       count_threshold: 10
#     - size_threshold_bytes: "1MB"
#       count_threshold: 10
#     - size_threshold_bytes: "10MB"
#       count_threshold: 10
#     - size_threshold_bytes: "100MB"
#       count_threshold: 10
#   resources:
#     cpu: "4"
#     memory: "8Gi"
#   autoscaling:
#     minReplicas: 2
#     maxReplicas: 4
#     memAverage: 50
#     cpuAverage: 50
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1
#   scratchVolume:
#     storageClassName: my-storage-class
#     storage: 1Gi
#     labels:
#       type: my-volume
#
## Optional query worker deployment
# logfire-ff-query-worker:
#   enabled: true
#   replicas: 8
#   resources:
#     cpu: "2"
#     memory: "2Gi"
#   autoscaling:
#     minReplicas: 8
#     maxReplicas: 128
#     memAverage: 20
#     cpuAverage: 50
#   pdb:
#     maxUnavailable: 1
#     minAvaliable: 1

# -- Object storage details
objectStore:
  # -- Uri for object storage i.e, `s3://bucket`
  uri:
  # -- additional env vars for the object store connection
  env: {}

# Azure
# objectStore:
#   uri: az://<container_name>
#   env:
#     AZURE_STORAGE_ACCOUNT_NAME: <storage_account_name>
#     AZURE_STORAGE_ACCOUNT_KEY:
#       valueFrom:
#         secretKeyRef:
#           name: my-azure-secret
#           key: account-key
# objectStoreEnv:
# AWS
# objectStore:
#   uri: s3://<bucket_name>
#   env:
#     AWS_DEFAULT_REGION: <region>
#     AWS_SECRET_ACCESS_KEY:
#       valueFrom:
#         secretKeyRef:
#           name: my-aws-secret
#           key: secret-key
#     AWS_ACCESS_KEY_ID: <access_key>

# -- the Kubernetes Service Account that is used by the pods
serviceAccountName: default

# -- Specify a priority class name to set [pod priority](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority).
priorityClassName: ""

# -- Define the [count of deployment revisions](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy) to be kept.
# May be set to 0 in case of GitOps deployment approach.
revisionHistoryLimit: 2

# -- Pod [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod).
# See the [API reference](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context) for details.
podSecurityContext: {}
  # fsGroup: 2000

# -- Container [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container).
# See the [API reference](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1) for details.
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # runAsNonRoot: true
  # runAsUser: 1000
  # readOnlyRootFilesystem: true

image:
  # -- The pull policy for docker images
  pullPolicy: IfNotPresent
  # @ignored
  repository: us-docker.pkg.dev/pydantic-public-registries/logfire/
  # @ignored
  workerImage: python-prod
  # @ignored
  backendImage: python-prod
  # @ignored
  frontendImage: frontend
  # @ignored
  schedulerImage: python-prod
  # @ignored
  fusionfireImage: fusionfire
  # @ignored
  dexImage: dex

logfire-redis:
  # -- Enable redis as part of this helm chart
  enabled: true

# -- Config for otel-collector
otel_collector:
  prometheus:
    enabled: false
    port: 9090
    endpoint: "0.0.0.0"
    send_timestamp: true
    metric_expiration: 180m
    enable_open_metrics: true
    add_metric_suffixes: false
    resource_to_telemetry_conversion:
      enabled: true

# -- The DSN for redis.  Change from default if you have an external redis instance
redisDsn: redis://logfire-redis:6379

# -- Existing Secret with the following keys
#  logfire-dex-client-secret
#  logfire-meta-write-token
#  logfire-meta-frontend-token
#  logfire-jwt-secret
existingSecret:
  enabled: false
  name: ""

# -- Development mode settings
dev:
  # -- Deploy maildev for testing emails
  deployMaildev: false
  # -- Deploy internal postgres
  # -- Do NOT use this in production!
  deployPostgres: false
  # -- Use a local MinIO instance as object storage
  # -- Do NOT use this in production!
  deployMinio: false
